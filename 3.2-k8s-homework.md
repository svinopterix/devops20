# K8s разворачивание кластера

```
vmedvedev@ubundesk:~/kubespray$ kubectl get nodes 
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   19h   v1.28.3
node2   Ready    control-plane   19h   v1.28.3
node3   Ready    control-plane   19h   v1.28.3
node4   Ready    <none>          19h   v1.28.3
node5   Ready    <none>          19h   v1.28.3
```

Для развертывания виртуалок использовал multipass (это такая утилитка от Canonical для запуска виртуалок с Ubuntu). 

```
vmedvedev@ubundesk:~/kubespray$ multipass list
Name                    State             IPv4             Image
kmast1                  Running           10.235.7.207     Ubuntu 22.04 LTS
                                          169.254.25.10
                                          10.233.0.1
                                          10.233.102.128
kmast2                  Running           10.235.7.149     Ubuntu 22.04 LTS
                                          169.254.25.10
                                          10.233.0.3
                                          10.233.75.0
kmast3                  Running           10.235.7.150     Ubuntu 22.04 LTS
                                          169.254.25.10
                                          10.233.0.1
                                          10.233.71.0
knode1                  Running           10.235.7.200     Ubuntu 22.04 LTS
                                          169.254.25.10
                                          10.233.0.1
                                          10.233.74.64
knode2                  Running           10.235.7.33      Ubuntu 22.04 LTS
                                          169.254.25.10
                                          10.233.0.3
                                          10.233.97.128
```

Разворачивал в HA-конфигруации, для чего добавил в файл inventory/devops20cluster/group_vars/all/all.yml следующие параметры:

```
## Internal loadbalancers for apiservers
loadbalancer_apiserver_localhost: true
loadbalancer_apiserver_type: nginx  # valid values "nginx" or "haproxy"
```

В результате на всех нодах, которые не control plane, поднялся nginx с балансировкой между нодами control plane. Ниже фрагмент автоматически сгенерированного конфига:
```
stream {
  upstream kube_apiserver {
    least_conn;
    server 10.235.7.207:6443;
    server 10.235.7.149:6443;
    server 10.235.7.150:6443;
    }

  server {
    listen        127.0.0.1:6443;
    proxy_pass    kube_apiserver;
    proxy_timeout 10m;
    proxy_connect_timeout 1s;
  }
}

http {

......

  server {
    listen 8081;
    location /healthz {
      access_log off;
      return 200;
    }
    location /stub_status {
      stub_status on;
      access_log off;
    }
  }
  }
```

Ниже результат запроса к одной из нод для проверки работы Nginx.

```
vmedvedev@ubundesk:~/kubespray$ curl -v http://10.235.7.200:8081/healthz
*   Trying 10.235.7.200:8081...
* Connected to 10.235.7.200 (10.235.7.200) port 8081 (#0)
> GET /healthz HTTP/1.1
> Host: 10.235.7.200:8081
> User-Agent: curl/7.81.0
> Accept: */*
> 
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx
< Date: Sat, 21 Oct 2023 14:54:08 GMT
< Content-Type: text/plain
< Content-Length: 0
< Connection: keep-alive
< 
* Connection #0 to host 10.235.7.200 left intact
```